{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests: Presidential Contributions\n",
    "\n",
    "Let's look at a random forests models for the presidential dataset.\n",
    "\n",
    "This dataset defines all presidential contribution amounts from publicly available information.\n",
    "\n",
    "The purpose here is to try to predict the amount that a candidate will give.\n",
    "\n",
    "Here are the feature columns we will use:\n",
    "1. Last Name (converted from Contributor Name)\n",
    "2. First Name (converted from Contributor Name)\n",
    "3. State \n",
    "4. Latitude (converted from Zipcode)\n",
    "5. Longitude (converted from zipcode)\n",
    "6. Employer\n",
    "7. Occupation\n",
    "\n",
    "### Notes\n",
    "\n",
    "This is going to be a very difficult dataset to get high accuracy, because we don't have any features that are highly correlated with the outcome. Part of our analysis is to see which features prove to be the most useful. \n",
    "\n",
    "One might suspect that information like State, might be very predictive -- because presumably New Yorkers might contribute to Hillary Clinton and Texans might contribute to Donald Trump. However, it turns out that State is pretty weakly correlated to the outcome.  \n",
    "\n",
    "One nice thing about random forests is that since we \"bag\" featues in differnet trees, we can empirically see which variables have hte most predictive power.  This is helpful for analytical reasons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "dataset = pd.read_csv(\"/data/presidential_election_contribs/2016/2016-medium-clean.csv\",)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(\"read {:,} records in {:,.2f} ms\".format(len(dataset), (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_column = ['CONTB_RECEIPT_AMT']\n",
    "numeric_columns = ['LAT', 'LNG']\n",
    "feature_columns = ['LASTNAME', 'FIRSTNAME', 'CONTBR_ST', 'LAT', 'LNG', 'CONTBR_EMPLOYER', \"CONTBR_OCCUPATION\"]\n",
    "categorical_columns = ['CAND_NM', 'LASTNAME', 'FIRSTNAME', 'CONTBR_ST', 'CONTBR_EMPLOYER', \"CONTBR_OCCUPATION\"]\n",
    "categorical_index = ['CAND_NM_index', 'FIRSTNAME_index', 'LASTNAME_index', 'CONTBR_ST_index', 'CONTBR_EMPLOYER_index', \n",
    "                     \"CONTBR_OCCUPATION_index\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out a contribution count broken down by candidate?\n",
    "**=> Q : Which candidates got the most donations? (in terms of number of donors) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : print out per candidate breakdown\n",
    "## Hint : What column represents Candidate name\n",
    "dataset.groupby('???').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : sort the output by number of contributions\n",
    "dataset.groupby('???').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Indexers and feature vector\n",
    "\n",
    "Let's index all the categorical columns, and build a labeld index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    dataset[col + '_index'] = pd.factorize(dataset[col])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ??? # numeric columns plus our *_index columns\n",
    "label = ??? #  What are we trying to predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split data into training and test\n",
    "\n",
    "**=> TODO: build training and test datasets 70%/30% **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : split 70% training and 30% testing\n",
    "## Hint : 0.7 ,  0.3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x,train_y, test_y = train_test_split(features, \n",
    "                                                    label, test_size=0.3)\n",
    "print(\"training set = \" , len(train_x))\n",
    "print(\"testing set = \" , len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting model training....this will take some time\")\n",
    "t1 = time.perf_counter()\n",
    "## TODO : train the model with our training set\n",
    "## Hint : training\n",
    "rf.fit(train_x,train_y)\n",
    "t2 = time.perf_counter()\n",
    "print(\"trained on {:,} records  in {:,.2f} ms\".\\\n",
    "      format(len(train_x),  (t2-t1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaulate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO: Think about the test error here?  Does it seem high?  What does that say about our model?**\n",
    "\n",
    "**=> How do we define model success? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Print the feature importanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO Compare the relative weight of the feature importances? **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature importances\n",
    "\n",
    "TODO: DO a visualization of feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> TODO Compare the relative weight of the feature importances? **\n",
    "\n",
    "Why do you think that the lat/long and other fields did not contribute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=> BONUS: Do a Pearson Correlation Matrix of the variables to the outcome, to see correlation **\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
