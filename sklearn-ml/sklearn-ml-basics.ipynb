{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Basics\n",
    "\n",
    "We are going to go over a few ML Basics to get the basic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Data\n",
    "Quick way to understand data set very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/college-admissions/admission-data.csv\")\n",
    "df\n",
    "\n",
    "# use describe() on all columns\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use describe on one column : GRE\n",
    "print(df['gre'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Training & Testing\n",
    "Run the following cell a few times, and observe the test / train sets.\n",
    "Each run will have differnet data for train/test.\n",
    "\n",
    "Q : How can we always get the same data for training and test?\n",
    "hint : Set the seed value to any integer   \n",
    "df.randomSplit (weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : let's split 70% for training and 30% for testing\n",
    "##    - first argument for randomSPlit is : 0.7  (representing 70%)\n",
    "##    - second argument for randomSPlit is : 0.3  (representing 30%)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "train_x,train_y,test_x, test_y = train_test_split(np.arange(200),\n",
    "                                                  np.arange(200),\n",
    "                                                  test_size=0.3)\n",
    "\n",
    "print(\"----training data set-----\")\n",
    "print(\"count: \", len(train_x))\n",
    "print(train_x)\n",
    "\n",
    "print(\"----testing data set-----\")\n",
    "print(\"count: \", len(test_x))\n",
    "print(test_x)\n",
    "\n",
    "## There should NO common data between training and test\n",
    "common = np.intersect1d(train_x, test_x)\n",
    "print(\"----common data set-----\")\n",
    "print(\"count: \", len(common))\n",
    "print(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now let's split a 'real world dataset'\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"/data/college-admissions/admission-data.csv\")\n",
    "\n",
    "## TODO : split training 80%,  testing 20%\n",
    "## Hint : arguments are 0.8  and 0.2\n",
    "train_x,train_y,test_x, test_y = train_test_split(np.arange(200),\n",
    "                                                  np.arange(200),\n",
    "                                                  test_size=0.3)\n",
    "\n",
    "\n",
    "print(\"----training data set-----\")\n",
    "print(\"count: \", len(train_x))\n",
    "training.show()\n",
    "\n",
    "print(\"----testing data set-----\")\n",
    "print(\"count: \", len(test_x))\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : evaluate how the data is split by 'admit' column\n",
    "## Hint : groupBy('admit')\n",
    "print(\"training data split\")\n",
    "training.groupBy(\"admit\").size()\n",
    "\n",
    "print(\"testing data split\")\n",
    "test.groupBy(\"admit\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## String Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas df\n",
    "import pandas as pd\n",
    "\n",
    "df_pd = pd.DataFrame({\"id\":[1,2,3,4,5,6,7], \n",
    "                      \"color\":['red', 'white', 'blue', 'blue', 'white' ,'yellow', 'blue' ]})\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pd['color_index'], colors = pd.factorize(df_pd['color'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if colors matches\n",
    "\n",
    "colors[df_pd['color_index']] == df_pd['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : create a pandas df\n",
    "import pandas as pd\n",
    "\n",
    "df2_pd = pd.DataFrame({\"id\":[1,2,3,4,5,6,7], \n",
    "                      \"status\":['married', 'single', 'single', 'divorced', 'married' ,'single', 'married' ]})\n",
    "df2_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2 : convert  categorical data to indexes \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3 : encode the indexes into a vector\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"statusIndex\", outputCol=\"statusVector\", dropLast=False)\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()\n",
    "\n",
    "# View dense vectors in pandas\n",
    "encoded_pd = encoded.toPandas()\n",
    "print(encoded_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Scaling Data\n",
    "\n",
    "### StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: create a pandas df\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_pd = pd.DataFrame({\"home_runs\": [ 30,  22,  17,  12, 44,   38,  40], \n",
    "                      \"salary_in_k\":[ 700, 450,340, 250, 1200, 800, 950 ]})\n",
    "df_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3 : Scale data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(df_pd)\n",
    "pd.DataFrame(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4 : Try a MinMaxScaler\n",
    "from sklearn.processing import MinMaxScaler\n",
    "\n",
    "## TODO : define minMaxScaler with  min=1  and max=100\n",
    "mmScaler = MinMaxScaler(feature_range=(???, ???))\n",
    "mmScaled = mmScaler.fit_transform(df_pd)\n",
    "pd.DataFrame(mmScaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
